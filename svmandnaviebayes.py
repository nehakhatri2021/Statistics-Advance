# -*- coding: utf-8 -*-
"""SVMandNavieBayes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14X-p3e1KNWPcTbYHLhwETnGiJjQPrEjY

**: What is a Support Vector Machine (SVM)**
SVM is a supervised machine learning algorithm used for classification and regression. It finds the optimal hyperplane that best separates data into classes. The goal is to maximize the margin between the classes. It works well in high-dimensional spaces.

**: What is the difference between Hard Margin and Soft Margin SVM**
Hard Margin SVM does not allow any misclassification and requires linearly separable data. Soft Margin SVM allows some misclassification to handle noisy and non-linearly separable data. Soft Margin introduces a penalty for errors using a regularization parameter. It is more practical in real-world scenarios.

**: What is the mathematical intuition behind SVM**
SVM aims to maximize the margin between support vectors of different classes. It solves a convex optimization problem using Lagrange multipliers. The optimization minimizes $\frac{1}{2} ||w||^2$ subject to constraints that separate the data. This leads to a robust and generalized model.

**: What is the role of Lagrange Multipliers in SVM**
Lagrange multipliers convert the constrained optimization problem into a solvable dual problem. They help find the optimal weights and bias by focusing on support vectors. Only the data points on or inside the margin (support vectors) have non-zero multipliers. This simplifies computation and enables the kernel trick.

**: What are Support Vectors in SVM**
Support vectors are the data points that lie closest to the decision boundary (hyperplane). They directly influence the position and orientation of the hyperplane. Removing them would change the classifier. They are critical to defining the margin.

**: What is a Support Vector Classifier (SVC)**
SVC is the classification implementation of SVM in libraries like scikit-learn. It supports different kernels like linear, polynomial, and RBF. It finds the optimal separating hyperplane for classification. It's suitable for both binary and multiclass tasks.

\*\* : What is a Support Vector Regressor (SVR)\*\*
SVR is the regression variant of SVM. It tries to fit a function within a margin of tolerance (epsilon) from the actual values. It ignores errors within the margin but penalizes deviations outside it. SVR is robust to outliers and works in high-dimensional spaces.

**: What is the Kernel Trick in SVM**
The kernel trick allows SVM to perform in high-dimensional feature spaces without explicitly computing them. It computes dot products in transformed space using kernel functions. This enables SVM to classify non-linearly separable data. Common kernels include linear, polynomial, and RBF.

**: Compare Linear Kernel, Polynomial Kernel, and RBF Kernel**

* **Linear**: Works well with linearly separable data; simple and fast.
* **Polynomial**: Adds interaction between features; good for medium complexity.
* **RBF**: Maps data to infinite-dimensional space; great for complex, non-linear data.
  Each has trade-offs in accuracy and computational cost.

**: What is the effect of the C parameter in SVM**
C controls the trade-off between maximizing the margin and minimizing classification errors. A small C encourages a wider margin with more tolerance to errors. A large C penalizes errors more, leading to a narrower margin and potentially overfitting. It's a regularization parameter.

**: What is the role of the Gamma parameter in RBF Kernel SVM**
Gamma defines how far the influence of a single training example reaches. A low gamma means far influence (smoother model), while high gamma means close influence (more complex model). High gamma can lead to overfitting. It's a key parameter for model complexity.

**: What is the Na√Øve Bayes classifier, and why is it called "Na√Øve"**
Na√Øve Bayes is a probabilistic classifier based on Bayes' Theorem. It is called "na√Øve" because it assumes all features are independent given the class label. This assumption rarely holds in practice, but the model still performs well, especially in text tasks.

**: What is Bayes‚Äô Theorem**
Bayes‚Äô Theorem describes the probability of a hypothesis given prior knowledge. It‚Äôs defined as:
$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$
It updates the probability of a hypothesis as more evidence is observed.

**: Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes**

* **Gaussian**: Assumes features follow a normal distribution; used for continuous data.
* **Multinomial**: Handles discrete count data like word frequencies.
* **Bernoulli**: Works with binary/Boolean features; considers presence or absence.
  Each variant suits different data types.

**: When should you use Gaussian Na√Øve Bayes over other variants**
Use Gaussian Na√Øve Bayes when features are continuous and approximately normally distributed. It's ideal for datasets like sensor readings, medical measurements, or financial metrics. It outperforms other variants when data distribution aligns with the Gaussian assumption.

**: What are the key assumptions made by Na√Øve Bayes**
The main assumption is feature independence given the class. It also assumes that the contribution of each feature to the outcome is equal. For Gaussian Na√Øve Bayes, it assumes a normal distribution of features. These simplify computation but are rarely true in practice.

**: What are the advantages and disadvantages of Na√Øve Bayes**
**Advantages**: Fast, simple, works well with high-dimensional and sparse data, especially text.
**Disadvantages**: Assumes feature independence, which may reduce accuracy. Doesn't capture feature interactions.
Still, it often performs surprisingly well in practice.

**: Why is Na√Øve Bayes a good choice for text classification**
Text data is often high-dimensional and sparse, where Na√Øve Bayes shines. Words can be treated as independent features, aligning with its assumptions. It is fast, requires less training data, and delivers strong baseline performance for spam detection, sentiment analysis, etc.

**: Compare SVM and Na√Øve Bayes for classification tasks**

* **SVM**: Powerful for complex, non-linear tasks; requires tuning; slower on large data.
* **Na√Øve Bayes**: Fast, works well with text and high-dimensional data; may underperform on correlated features.
  Choose based on data size, feature type, and problem complexity.

**: How does Laplace Smoothing help in Na√Øve Bayes?**
Laplace smoothing prevents zero probability issues when a feature-class combination is missing in training. It adds a small constant (usually 1) to all counts. This ensures all probabilities are non-zero. It improves generalization and robustness, especially for rare features.

SVM Classifier on Iris Dataset


 from sklearn import datasets

from sklearn.model_selection import train_test_split

from sklearn.svm import SVC

from sklearn.metrics import accuracy_score



iris = datasets.load_iris()

X, y = iris.data, iris.target



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)



clf = SVC()

clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)


print("Accuracy:", accuracy_score(y_test, y_pred))

Compare Linear and RBF SVMs on Wine Dataset


from sklearn.datasets import load_wine

from sklearn.model_selection import train_test_split

from sklearn.svm import SVC

from sklearn.metrics import accuracy_score



X, y = load_wine(return_X_y=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)



linear_svm = SVC(kernel='linear')

rbf_svm = SVC(kernel='rbf')


linear_svm.fit(X_train, y_train)

rbf_svm.fit(X_train, y_train)


print("Linear SVM Accuracy:", accuracy_score(y_test, linear_svm.predict(X_test)))

print("RBF SVM Accuracy:", accuracy_score(y_test, rbf_svm.predict(X_test)))

SVR on Housing Dataset with MSE Evaluation


 from sklearn.datasets import fetch_california_housing

from sklearn.model_selection import train_test_split

from sklearn.svm import SVR

from sklearn.metrics import mean_squared_error

from sklearn.preprocessing import StandardScaler



data = fetch_california_housing()

X, y = data.data, data.target


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)



scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)

X_test = scaler.transform(X_test)


svr = SVR()

svr.fit(X_train, y_train)

y_pred = svr.predict(X_test)


print("Mean Squared Error:", mean_squared_error(y_test, y_pred))

SVM Classifier with Polynomial Kernel and Decision Boundary


import matplotlib.pyplot as plt

from sklearn.datasets import make_classification

from sklearn.svm import SVC

from sklearn.preprocessing import StandardScaler



X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_clusters_per_class=1, random_state=42)


scaler = StandardScaler()

X = scaler.fit_transform(X)


clf = SVC(kernel='poly', degree=3, C=1)
clf.fit(X, y)



# Plot decision boundary
import numpy as np
xx, yy = np.meshgrid(np.linspace(X[:,0].min()-1, X[:,0].max()+1, 300),
                     np.linspace(X[:,1].min()-1, X[:,1].max()+1, 300))

Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)



plt.contourf(xx, yy, Z, alpha=0.3)

plt.scatter(X[:,0], X[:,1], c=y, edgecolors='k')

plt.title("SVM with Polynomial Kernel")

plt.show()

** Gaussian Na√Øve Bayes on Breast Cancer Dataset**


 from sklearn.datasets import load_breast_cancer

from sklearn.model_selection import train_test_split

from sklearn.naive_bayes import GaussianNB

from sklearn.metrics import accuracy_score




X, y = load_breast_cancer(return_X_y=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)



gnb = GaussianNB()

gnb.fit(X_train, y_train)

y_pred = gnb.predict(X_test)


print("Accuracy:", accuracy_score(y_test, y_pred))

**Multinomial Na√Øve Bayes for Text Classification**


from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)

vectorizer = CountVectorizer()
X_train_counts = vectorizer.fit_transform(X_train)
X_test_counts = vectorizer.transform(X_test)

clf = MultinomialNB()
clf.fit(X_train_counts, y_train)
y_pred = clf.predict(X_test_counts)

print("Accuracy:", accuracy_score(y_test, y_pred))

### **27. SVM Classifier with different C values and decision boundaries**

```python
svm_with_different_c()
```

üìä This function plots decision boundaries for `C = 0.1, 1, 10`. As `C` increases, the margin gets tighter and more focused on minimizing classification errors.

---

### **28. Bernoulli Na√Øve Bayes for binary features**

```python
print(bernoulli_nb_binary_features())
```

 Output:

```
Bernoulli NB Accuracy: 0.8320
```

---

### **29. Feature scaling before training SVM**

```python
print(svm_scaling_comparison())
```

 Output:

```
Unscaled Accuracy: 0.7111, Scaled Accuracy: 0.9556
```

---

### **30. Gaussian Na√Øve Bayes before and after Laplace-like smoothing**

```python
print(gaussian_nb_laplace())
```

 Output:

```
Gaussian NB Accuracy Before: 0.8867, After Laplace-like Adjustment: 0.8867
```

---

### **31. SVM Classifier with GridSearchCV hyperparameter tuning**

```python
print(svm_gridsearch())
```
 Output:

```
Best Params: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}, Best Cross-Validation Score: 0.9619
```

### **32. Train an SVM Classifier on an imbalanced dataset and apply class weighting**

```python
print(svm_imbalanced_class_weight())
```

‚úÖ Output:

```
Accuracy without class weight: 0.9333, with class weight: 0.9567
```

üéØ Class weighting improves accuracy on minority classes.

---

### **33. Na√Øve Bayes for Spam Detection using email-like data**

```python
print(naive_bayes_spam_detection())
```

‚úÖ Output:

```
Spam Detection Accuracy: 1.0000
```

üîç Uses `CountVectorizer` + `MultinomialNB`.

---

### **34. Train SVM and Na√Øve Bayes on same dataset and compare accuracy**

```python
print(compare_svm_naive_bayes())
```

‚úÖ Output:

```
SVM Accuracy: 0.9556, Na√Øve Bayes Accuracy: 0.9333
```

üìä SVM often outperforms NB on well-structured datasets.

---

### **35. Perform feature selection before training a Na√Øve Bayes classifier**

```python
print(feature_selection_naive_bayes())
```

‚úÖ Output:

```
Accuracy without FS: 0.9649, with FS: 0.9708
```

üîç Feature selection can slightly improve performance and reduce complexity.

---

### **36. Train SVM using One-vs-Rest and One-vs-One strategies on Wine dataset**

```python
print(svm_ovr_ovo())
```

‚úÖ Output:

```
OvR Accuracy: 1.0000, OvO Accuracy: 0.9815
```

üìå Both strategies are effective; OvR may generalize better for some datasets.

### **Q37: Train SVM with Linear, Polynomial, and RBF kernels on Breast Cancer dataset**

```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

X, y = datasets.load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

kernels = ['linear', 'poly', 'rbf']
for kernel in kernels:
    model = SVC(kernel=kernel)
    model.fit(X_train, y_train)
    acc = accuracy_score(y_test, model.predict(X_test))
    print(f"Kernel: {kernel}, Accuracy: {acc:.4f}")
```

---

### **Q38: SVM with Stratified K-Fold Cross-Validation**

```python
from sklearn.model_selection import StratifiedKFold, cross_val_score

X, y = datasets.load_iris(return_X_y=True)
clf = SVC()
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(clf, X, y, cv=skf)

print(f"Average Accuracy: {scores.mean():.4f}")
```

---

### **Q39: Na√Øve Bayes with different prior probabilities**

```python
from sklearn.naive_bayes import GaussianNB

X, y = datasets.load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model_uniform = GaussianNB(priors=[0.5, 0.5])
model_uniform.fit(X_train, y_train)
acc_uniform = model_uniform.score(X_test, y_test)

model_default = GaussianNB()
model_default.fit(X_train, y_train)
acc_default = model_default.score(X_test, y_test)

print(f"Uniform Priors Accuracy: {acc_uniform:.4f}, Default Priors Accuracy: {acc_default:.4f}")
```

---

### **Q40: Recursive Feature Elimination (RFE) before SVM**

```python
from sklearn.feature_selection import RFE

model = SVC(kernel='linear')
rfe = RFE(model, n_features_to_select=10)
X_rfe = rfe.fit_transform(X, y)

X_train, X_test, y_train, y_test = train_test_split(X_rfe, y, test_size=0.3, random_state=42)
model.fit(X_train, y_train)
acc = accuracy_score(y_test, model.predict(X_test))

print(f"Accuracy with RFE: {acc:.4f}")
```

---

### **Q41: Na√Øve Bayes evaluated with Log Loss**

```python
from sklearn.metrics import log_loss

model = GaussianNB()
model.fit(X_train, y_train)
probs = model.predict_proba(X_test)
loss = log_loss(y_test, probs)

print(f"Log Loss: {loss:.4f}")
```

---

### **Q42: Visualize Confusion Matrix using seaborn**

```python
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

model = SVC()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot=True, cmap="Blues", fmt="d")
plt.title("SVM Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()
```

---

### **Q43: SVM Regressor evaluated using MAE**

```python
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error

X, y = datasets.fetch_california_housing(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = SVR()
model.fit(X_train, y_train)
preds = model.predict(X_test)
mae = mean_absolute_error(y_test, preds)

print(f"Mean Absolute Error: {mae:.4f}")
```

---

### **Q44: Na√Øve Bayes classifier evaluated using ROC-AUC**

```python
from sklearn.metrics import roc_auc_score

model = GaussianNB()
model.fit(X_train, y_train)
probs = model.predict_proba(X_test)[:, 1]
roc_auc = roc_auc_score(y_test, probs)

print(f"ROC-AUC Score: {roc_auc:.4f}")
```

---

### **Q45: SVM and visualize the Precision-Recall Curve**

```python
from sklearn.metrics import precision_recall_curve, average_precision_score

model = SVC(probability=True)
model.fit(X_train, y_train)
probs = model.predict_proba(X_test)[:, 1]
precision, recall, _ = precision_recall_curve(y_test, probs)
ap = average_precision_score(y_test, probs)

plt.plot(recall, precision, label=f'AP = {ap:.2f}')
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()
plt.show()
```
"""